{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c670533f",
   "metadata": {},
   "source": [
    "# Generating a dataset of concepts\n",
    "- seeded on [Amazon Books Reviews](https://www.kaggle.com/datasets/mohamedbakhet/amazon-books-reviews) data but filtered as shown below \n",
    "- We augment Authors and Topics with Wiki crawls\n",
    "- We generate fake demograhpic data to enhance users using [Telco dataset](https://www.kaggle.com/datasets/blastchar/telco-customer-churn)\n",
    "- We also generate a fake univesity relationship from a [world university rankings](https://www.kaggle.com/datasets/mylesoneill/world-university-rankings)\n",
    "- We do some named entity recongnition in long texsts and do a wiki crawl for random concepts\n",
    "\n",
    "![Data](GenDataset.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144fea6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_OUT = '/Users/sirsh/Downloads/monologue_dataset'\n",
    "#!mkdir /Users/sirsh/Downloads/monologue_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ee18e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from stringcase import snakecase\n",
    "from ast import literal_eval\n",
    "df = pd.read_csv(\"/Users/sirsh/Downloads/uniranking/timesData.csv\",on_bad_lines='skip')\n",
    "df = df.replace('-',-1)\n",
    "\n",
    "#take the best without null values\n",
    "top_unis = df[:250].dropna()[:200].reset_index(drop=True)\n",
    "score_fields = ['teaching','international','research','citations','student_staff_ratio', 'total_score', 'income']\n",
    "for f in score_fields:\n",
    "    top_unis[f] = top_unis[f].map(float)\n",
    "top_unis['pct_international_students'] = top_unis['international_students'].map(lambda x: float(x.replace('%','')))\n",
    "top_unis['num_students'] = top_unis['num_students'].map(lambda x: int(float(x.replace(',','')))).map(int)\n",
    "top_unis['world_rank'] = top_unis['world_rank'].map(int)\n",
    "top_unis['year'] = top_unis['year'].map(int)\n",
    "top_unis.to_feather(f\"{DATA_OUT}/unis.feather\")\n",
    "top_unis.to_csv(f\"{DATA_OUT}/unis.csv\")\n",
    "top_unis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0302dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556317d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(top_unis.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3459b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#considered generating names from https://www.kaggle.com/datasets/kaggle/us-baby-names but not bothered\n",
    "#pd.read_csv(\"/Users/sirsh/Downloads/names_us/StateNames.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48245e31",
   "metadata": {},
   "source": [
    "# Amazon book reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8a371c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetched from the mentioned dataset\n",
    "df = pd.read_csv(\"/Users/sirsh/Downloads/amazon/books_data.csv\",on_bad_lines='skip')\n",
    "\n",
    "df = df.dropna(subset=['authors', 'categories', 'publishedDate', 'publisher'])\n",
    "df['first_author'] = df['authors'].map(lambda x : literal_eval(x)[0])\n",
    "df['category'] = df['categories'].map(lambda x : literal_eval(x)[0])\n",
    "clean_books = df[['Title', 'description', 'first_author', 'category', 'publishedDate', 'previewLink', 'image']]\n",
    "clean_books.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bcd6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/sirsh/Downloads/amazon/Books_rating.csv\",on_bad_lines='skip')\n",
    "\n",
    "reviewers_count = df[df['User_id'].notnull()].groupby('User_id').count().sort_values('Id')\n",
    "frequent_reviewers = reviewers_count[reviewers_count['Id']>5].index.values\n",
    "\n",
    "review_count = df.groupby('Title').count().sort_values('Id')\n",
    "review_count = review_count.reset_index()[['Title','Id']].rename(columns={'Id': \"approx_review_count\"})\n",
    "df = pd.merge(df, review_count, on='Title')\n",
    "#keep what has been reviewed often\n",
    "df = df[df['approx_review_count']>1000]\n",
    "#keep what is in our list of cleaner books\n",
    "df = df[df['Title'].isin(clean_books['Title'])]\n",
    "#keep only records with reviewers that have reviewed more than 5 times as calc'd above\n",
    "df = df[df['User_id'].isin(frequent_reviewers)]\n",
    "df = df[['Id', 'Title', 'review/score', 'review/time','review/text', 'profileName' ,'User_id' ]].reset_index(drop=True)\n",
    "\n",
    "df.columns = [snakecase(c).replace('/','_').replace('__','_').lower() for c in df.columns]\n",
    "df.to_feather(f\"{DATA_OUT}/amz_book_reviews_curated.feather\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e113179c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep only what we referenced\n",
    "clean_books = clean_books[clean_books['Title'].isin(df['title'].unique())].reset_index(drop=True)\n",
    "clean_books.columns = [snakecase(c).replace('/','_').lower() for c in clean_books.columns]\n",
    "#be careful of mixed types\n",
    "clean_books['published_date'] = clean_books['published_date'].map(str)\n",
    "clean_books.to_feather(f\"{DATA_OUT}/amz_books_curated.feather\")\n",
    "clean_books"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416cb75b",
   "metadata": {},
   "source": [
    "# Generating Fake Users\n",
    "- we give the users mentioned in the veview some color\n",
    "- a univesity\n",
    "- a favourite X from some know lists "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22b8048",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorized_book_reviews = pd.merge(df, clean_books[['title','category']], on='title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080b596c",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_status = categorized_book_reviews.groupby('user_id').agg({'id': len, 'review_score': [min,max,np.mean], 'profile_name':min})\n",
    "user_status.columns =  ['review_count', 'review_min', 'review_max', 'review_average', 'profile_name'] \n",
    "fav_books = categorized_book_reviews.groupby('user_id')['title'].apply(lambda x: x.value_counts().idxmax())\n",
    "fav_category = categorized_book_reviews.groupby('user_id')['category'].apply(lambda x: x.value_counts().idxmax())\n",
    "users = user_status.join(fav_books).join(fav_category).reset_index().rename(columns={\n",
    "    \"title\": 'favourite_book',\n",
    "    \"category\": 'favourite_topic'\n",
    "})\n",
    "users['university_attended'] = top_unis['university_name'].sample(len(users),replace=True).values\n",
    "users.to_feather(f\"{DATA_OUT}/amz_books_reviewer_users_generated.feather\")\n",
    "users\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f038913",
   "metadata": {},
   "source": [
    "# Generating Concepts\n",
    "- When generating concepts we decide on specificity\n",
    "- For example we can define People or Authors. Authors can be just people with a \"type\" \n",
    "- We had relations e.g. related concept links which is good for traversal \n",
    "- The core entity can be added to redis (without the text) or to vector stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a213741c",
   "metadata": {},
   "outputs": [],
   "source": [
    "users['favourite_topic'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6ab00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "list(glob(f\"{DATA_OUT}/*.*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10db3ead",
   "metadata": {},
   "source": [
    "# How monologue ingests data with the correct schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf803bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "sys.path.append(\"../../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b9bb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monologue.core.data.stores import VectorDataStore, ColumnarDataStore\n",
    "from monologue.entities.examples import TopUniversities, Books, BookReviewers, AbstractVectorStoreEntry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fb15b1",
   "metadata": {},
   "source": [
    "### first the columnar store version - simply writes to parquet under the hood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75ba879",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ingest /merge data\n",
    "ColumnarDataStore.ingest_records('/Users/sirsh/Downloads/monologue_dataset/amz_books_curated.feather', \n",
    "                                 entity_type=Books, \n",
    "                                 mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b648b904",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ask the tool questions\n",
    "books_store = ColumnarDataStore(Books)\n",
    "#books_store(\"List any books by Tolken?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a669451",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ingest - this is fast using parquet only\n",
    "ColumnarDataStore.ingest_records('/Users/sirsh/Downloads/monologue_dataset/unis.feather', entity_type=TopUniversities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b77e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = ColumnarDataStore(TopUniversities)\n",
    "#store(\"What university has the top citations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc08b26",
   "metadata": {},
   "source": [
    "## Now the vector store type\n",
    "- First we show how a generic wikipedia entry can be ingested into a \"dynamic type\"\n",
    "- here we make sue of Pydantic to create a type that inherits from another type\n",
    "- The rest works the same; ny known type can be ingested in this way - we have some pre-baked vector data types in the examples\n",
    "- the other thing to know is the wikipedia data maps intp the abstract vector type since it has name and text\n",
    "- the vector store type defaults some other things that can be set like id and doc_ids\n",
    "- the vector embeddings is computing on add (Using LanceDB and whatever embeddings are specified in the pydantic type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6385a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the WikiWalker to get some sample data\n",
    "from monologue.core.data.clients import WikiWalker\n",
    "from monologue.entities.examples import AbstractVectorStoreEntry\n",
    "for record in WikiWalker().iter_sections(\"Philosophy\"):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd81e0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this creates a vector store entry that will be under GeneralTopics\n",
    "#by inheriting from the abstract entity it is setup to wrte vector data and mebeddings\n",
    "#this is a very thin pydantic descriptor and LanceDB. Check it out under the hood.\n",
    "generic_topic = AbstractVectorStoreEntry.create_model(\"GeneralTopics\")\n",
    "#\n",
    "generic_topic(**record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a553db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = VectorDataStore(generic_topic)\n",
    "collection = [ generic_topic(**record) for record in WikiWalker().iter_sections(\"Philosophy\")]\n",
    "store.add(collection)\n",
    "store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4419296",
   "metadata": {},
   "outputs": [],
   "source": [
    "store(\"Where did the word Philosophy come from?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7c85f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
